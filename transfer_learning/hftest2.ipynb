{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import preprocess as ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "    # Tanulási folyamat elején létrehozunk egy-egy üres listát a kinyerni kívánt metrikák tárolása céljából.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Hiba mértéke a tanító adatokon.\n",
    "        self.losses = []\n",
    "        # Hiba mértéke a validációs adatokon.\n",
    "        self.valid_losses = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a tanító adatokon. \n",
    "        self.accs = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a validációs adatokon. \n",
    "        self.valid_accs = []\n",
    "        # A tanítási fázisok sorszámozása.\n",
    "        self.epoch = 0\n",
    "    \n",
    "    # Minden egyes tanítási fázis végén mentsük el, hogy hogyan teljesít aktuálisan a háló. \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 1 == 0:\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.valid_losses.append(logs.get('val_loss'))\n",
    "            self.accs.append(logs.get('acc'))\n",
    "            self.valid_accs.append(logs.get('val_acc'))\n",
    "            self.epoch += 1\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n"
     ]
    }
   ],
   "source": [
    "# epoch szám\n",
    "nb_epoch=10\n",
    "\n",
    "# előtanított modell betöltése, a fully-connected rétegek nélkül\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# az utolsó konvolúciós réteg utána egy global average pooling réteget teszünk, ez rögtön \"lapítja\" (flatten) a 2D konvolúciót\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# ezután hozzáadunk egy előrecsatolt réteget ReLU aktivációs függvénnyel\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# és végül egy kimenete lesz a hálónak - a \"binary_crossentropy\" költségfüggvénynek erre van szüksége\n",
    "predictions = Dense(14, activation='sigmoid')(x)\n",
    "# a model létrehozása\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# két lépésben fogjuk tanítani a hálót\n",
    "# az első lépésben csak az előrecsatolt rétegeket tanítjuk, a konvolúciós rétegeket befagyasztjuk\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# lefordítjuk a modelt (fontos, hogy ezt a rétegek befagyasztása után csináljuk\"\n",
    "# mivel két osztályunk van, ezért bináris keresztentrópia költségfüggvényt használunk\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[189 180 165]\n",
      "  [181 172 157]\n",
      "  [189 180 165]\n",
      "  ..., \n",
      "  [181 176 154]\n",
      "  [164 159 137]\n",
      "  [205 200 178]]\n",
      "\n",
      " [[197 188 173]\n",
      "  [191 182 167]\n",
      "  [189 180 165]\n",
      "  ..., \n",
      "  [180 175 153]\n",
      "  [184 179 157]\n",
      "  [217 212 190]]\n",
      "\n",
      " [[197 188 173]\n",
      "  [198 189 174]\n",
      "  [190 181 166]\n",
      "  ..., \n",
      "  [179 174 154]\n",
      "  [198 193 173]\n",
      "  [207 202 182]]\n",
      "\n",
      " ..., \n",
      " [[ 96  17  22]\n",
      "  [ 91  19  22]\n",
      "  [ 74  13  10]\n",
      "  ..., \n",
      "  [  3  18  11]\n",
      "  [  3  18  13]\n",
      "  [  5  20  15]]\n",
      "\n",
      " [[107  28  33]\n",
      "  [ 96  24  27]\n",
      "  [ 69   8   5]\n",
      "  ..., \n",
      "  [  7  20  13]\n",
      "  [ 10  23  16]\n",
      "  [ 14  27  20]]\n",
      "\n",
      " [[114  35  40]\n",
      "  [101  29  32]\n",
      "  [ 75  14  11]\n",
      "  ..., \n",
      "  [ 21  34  25]\n",
      "  [ 21  34  27]\n",
      "  [ 21  34  27]]]\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "[[[220 208 168]\n",
      "  [220 208 168]\n",
      "  [219 207 167]\n",
      "  ..., \n",
      "  [123 112  66]\n",
      "  [108  97  52]\n",
      "  [ 95  84  39]]\n",
      "\n",
      " [[222 210 170]\n",
      "  [222 210 170]\n",
      "  [221 209 169]\n",
      "  ..., \n",
      "  [117 106  61]\n",
      "  [101  90  45]\n",
      "  [ 88  77  32]]\n",
      "\n",
      " [[225 213 173]\n",
      "  [224 212 172]\n",
      "  [224 212 172]\n",
      "  ..., \n",
      "  [119 108  63]\n",
      "  [106  94  52]\n",
      "  [ 96  84  42]]\n",
      "\n",
      " ..., \n",
      " [[146 143  98]\n",
      "  [147 144  99]\n",
      "  [149 146 101]\n",
      "  ..., \n",
      "  [  3   5   2]\n",
      "  [  4   6   3]\n",
      "  [  5   7   4]]\n",
      "\n",
      " [[141 138  95]\n",
      "  [143 140  97]\n",
      "  [145 142  99]\n",
      "  ..., \n",
      "  [  3   5   2]\n",
      "  [  4   6   3]\n",
      "  [  5   7   4]]\n",
      "\n",
      " [[149 145 107]\n",
      "  [149 145 107]\n",
      "  [149 145 107]\n",
      "  ..., \n",
      "  [  4   6   3]\n",
      "  [  5   7   4]\n",
      "  [  6   8   5]]]\n",
      "[[[194 138  64]\n",
      "  [203 147  73]\n",
      "  [210 154  80]\n",
      "  ..., \n",
      "  [ 93  40   0]\n",
      "  [101  47   9]\n",
      "  [104  50  12]]\n",
      "\n",
      " [[204 148  74]\n",
      "  [209 153  79]\n",
      "  [209 153  79]\n",
      "  ..., \n",
      "  [188 135  91]\n",
      "  [176 123  81]\n",
      "  [167 114  72]]\n",
      "\n",
      " [[211 155  81]\n",
      "  [213 157  83]\n",
      "  [212 156  82]\n",
      "  ..., \n",
      "  [224 172 122]\n",
      "  [221 169 119]\n",
      "  [218 166 116]]\n",
      "\n",
      " ..., \n",
      " [[102  84  48]\n",
      "  [103  85  49]\n",
      "  [100  82  46]\n",
      "  ..., \n",
      "  [ 80  74  52]\n",
      "  [ 73  67  45]\n",
      "  [ 76  70  48]]\n",
      "\n",
      " [[ 89  79  52]\n",
      "  [ 91  81  54]\n",
      "  [ 90  78  52]\n",
      "  ..., \n",
      "  [ 91  80  60]\n",
      "  [ 73  62  42]\n",
      "  [ 92  81  61]]\n",
      "\n",
      " [[ 83  73  46]\n",
      "  [ 85  75  48]\n",
      "  [ 84  72  46]\n",
      "  ..., \n",
      "  [ 91  80  60]\n",
      "  [ 71  60  40]\n",
      "  [ 90  79  59]]]\n",
      "[[[251 233 213]\n",
      "  [251 233 213]\n",
      "  [251 233 213]\n",
      "  ..., \n",
      "  [250 235 212]\n",
      "  [250 235 212]\n",
      "  [250 235 212]]\n",
      "\n",
      " [[251 233 213]\n",
      "  [251 233 213]\n",
      "  [251 233 213]\n",
      "  ..., \n",
      "  [250 235 212]\n",
      "  [250 235 212]\n",
      "  [250 235 212]]\n",
      "\n",
      " [[250 233 213]\n",
      "  [250 233 213]\n",
      "  [250 233 213]\n",
      "  ..., \n",
      "  [250 235 212]\n",
      "  [250 235 212]\n",
      "  [250 235 212]]\n",
      "\n",
      " ..., \n",
      " [[251 236 217]\n",
      "  [251 236 217]\n",
      "  [251 236 217]\n",
      "  ..., \n",
      "  [248 237 219]\n",
      "  [248 237 219]\n",
      "  [248 237 219]]\n",
      "\n",
      " [[248 233 214]\n",
      "  [249 234 215]\n",
      "  [250 235 216]\n",
      "  ..., \n",
      "  [249 238 220]\n",
      "  [249 238 220]\n",
      "  [249 238 220]]\n",
      "\n",
      " [[248 233 214]\n",
      "  [249 234 215]\n",
      "  [250 235 216]\n",
      "  ..., \n",
      "  [249 238 220]\n",
      "  [249 238 220]\n",
      "  [249 238 220]]]\n",
      "[[[131 132  98]\n",
      "  [134 128  92]\n",
      "  [174 159 120]\n",
      "  ..., \n",
      "  [ 52  69  61]\n",
      "  [ 42  68  57]\n",
      "  [ 42  72  60]]\n",
      "\n",
      " [[128 126  88]\n",
      "  [125 117  78]\n",
      "  [191 174 131]\n",
      "  ..., \n",
      "  [ 41  58  50]\n",
      "  [ 36  59  49]\n",
      "  [ 36  64  52]]\n",
      "\n",
      " [[116 109  67]\n",
      "  [153 140  96]\n",
      "  [194 173 128]\n",
      "  ..., \n",
      "  [ 47  62  55]\n",
      "  [ 43  64  55]\n",
      "  [ 41  67  56]]\n",
      "\n",
      " ..., \n",
      " [[187 137  84]\n",
      "  [148  98  47]\n",
      "  [148 100  51]\n",
      "  ..., \n",
      "  [188 174 127]\n",
      "  [136 125  80]\n",
      "  [ 85  76  35]]\n",
      "\n",
      " [[161 116  77]\n",
      "  [167 122  83]\n",
      "  [159 114  73]\n",
      "  ..., \n",
      "  [235 208 165]\n",
      "  [210 187 143]\n",
      "  [ 70  51   8]]\n",
      "\n",
      " [[148 103  64]\n",
      "  [155 110  71]\n",
      "  [145 100  59]\n",
      "  ..., \n",
      "  [186 159 116]\n",
      "  [119  96  52]\n",
      "  [142 123  80]]]\n",
      "[[[ 95  80  83]\n",
      "  [ 73  58  61]\n",
      "  [ 76  61  64]\n",
      "  ..., \n",
      "  [243 234 237]\n",
      "  [240 231 234]\n",
      "  [241 232 235]]\n",
      "\n",
      " [[ 53  38  41]\n",
      "  [ 79  64  67]\n",
      "  [ 93  78  81]\n",
      "  ..., \n",
      "  [ 32  23  26]\n",
      "  [ 32  23  26]\n",
      "  [ 49  40  43]]\n",
      "\n",
      " [[ 85  73  75]\n",
      "  [ 70  58  60]\n",
      "  [ 73  61  63]\n",
      "  ..., \n",
      "  [ 49  40  43]\n",
      "  [ 56  47  50]\n",
      "  [ 65  56  59]]\n",
      "\n",
      " ..., \n",
      " [[219 204 211]\n",
      "  [ 46  31  38]\n",
      "  [ 67  54  61]\n",
      "  ..., \n",
      "  [ 38  38  40]\n",
      "  [ 38  38  40]\n",
      "  [ 50  50  52]]\n",
      "\n",
      " [[156 141 148]\n",
      "  [ 81  66  73]\n",
      "  [ 84  71  78]\n",
      "  ..., \n",
      "  [ 35  35  37]\n",
      "  [ 36  36  38]\n",
      "  [ 48  48  50]]\n",
      "\n",
      " [[166 149 157]\n",
      "  [ 87  72  79]\n",
      "  [ 68  55  62]\n",
      "  ..., \n",
      "  [ 27  27  29]\n",
      "  [ 32  32  34]\n",
      "  [ 49  49  51]]]\n",
      "[[[50 49 29]\n",
      "  [58 57 37]\n",
      "  [58 57 37]\n",
      "  ..., \n",
      "  [19 15 14]\n",
      "  [20 14 14]\n",
      "  [48 42 44]]\n",
      "\n",
      " [[45 44 26]\n",
      "  [53 52 34]\n",
      "  [88 87 69]\n",
      "  ..., \n",
      "  [17 13 12]\n",
      "  [21 15 17]\n",
      "  [39 33 35]]\n",
      "\n",
      " [[52 50 35]\n",
      "  [38 36 21]\n",
      "  [78 76 61]\n",
      "  ..., \n",
      "  [ 6  2  1]\n",
      "  [ 8  4  5]\n",
      "  [10  5  9]]\n",
      "\n",
      " ..., \n",
      " [[48 32 17]\n",
      "  [59 43 28]\n",
      "  [72 58 45]\n",
      "  ..., \n",
      "  [10  8 13]\n",
      "  [13  8 12]\n",
      "  [14  9 13]]\n",
      "\n",
      " [[75 58 48]\n",
      "  [21  7  0]\n",
      "  [55 41 30]\n",
      "  ..., \n",
      "  [ 9  7 10]\n",
      "  [19 15 16]\n",
      "  [ 8  4  5]]\n",
      "\n",
      " [[35 21 12]\n",
      "  [47 33 24]\n",
      "  [41 28 20]\n",
      "  ..., \n",
      "  [19 17 18]\n",
      "  [38 34 33]\n",
      "  [14  9  6]]]\n",
      "[[[ 98  72  35]\n",
      "  [ 96  76  43]\n",
      "  [ 97  81  56]\n",
      "  ..., \n",
      "  [ 66  71  65]\n",
      "  [ 55  60  54]\n",
      "  [ 57  62  56]]\n",
      "\n",
      " [[167 141 106]\n",
      "  [ 84  64  31]\n",
      "  [121 105  80]\n",
      "  ..., \n",
      "  [ 61  66  60]\n",
      "  [ 52  57  51]\n",
      "  [ 58  63  57]]\n",
      "\n",
      " [[197 171 136]\n",
      "  [112  92  59]\n",
      "  [111  95  70]\n",
      "  ..., \n",
      "  [ 57  62  56]\n",
      "  [ 52  57  51]\n",
      "  [ 61  66  60]]\n",
      "\n",
      " ..., \n",
      " [[137 114  80]\n",
      "  [149 126  92]\n",
      "  [141 118  84]\n",
      "  ..., \n",
      "  [240 199 143]\n",
      "  [239 200 143]\n",
      "  [234 195 138]]\n",
      "\n",
      " [[163 143 108]\n",
      "  [177 157 122]\n",
      "  [173 153 118]\n",
      "  ..., \n",
      "  [237 191 139]\n",
      "  [233 190 137]\n",
      "  [237 194 143]]\n",
      "\n",
      " [[151 131  96]\n",
      "  [160 140 105]\n",
      "  [153 133  98]\n",
      "  ..., \n",
      "  [236 188 139]\n",
      "  [234 188 138]\n",
      "  [238 194 145]]]\n",
      "[[[ 33  32  28]\n",
      "  [ 21  20  16]\n",
      "  [ 17  16  14]\n",
      "  ..., \n",
      "  [187  74  32]\n",
      "  [214  78  28]\n",
      "  [217  66  11]]\n",
      "\n",
      " [[ 38  37  33]\n",
      "  [ 31  30  26]\n",
      "  [ 31  30  28]\n",
      "  ..., \n",
      "  [211  94  61]\n",
      "  [237  95  55]\n",
      "  [216  61  15]]\n",
      "\n",
      " [[ 29  25  22]\n",
      "  [ 27  23  20]\n",
      "  [ 33  29  26]\n",
      "  ..., \n",
      "  [173  56  36]\n",
      "  [192  51  23]\n",
      "  [255 130  99]]\n",
      "\n",
      " ..., \n",
      " [[ 80  63  43]\n",
      "  [ 52  52  42]\n",
      "  [ 73  79  79]\n",
      "  ..., \n",
      "  [ 26  20  20]\n",
      "  [ 59  53  53]\n",
      "  [ 96  90  90]]\n",
      "\n",
      " [[ 75  59  36]\n",
      "  [ 82  79  70]\n",
      "  [ 53  62  61]\n",
      "  ..., \n",
      "  [ 38  32  32]\n",
      "  [ 92  86  86]\n",
      "  [ 96  90  90]]\n",
      "\n",
      " [[ 86  70  47]\n",
      "  [102  99  90]\n",
      "  [ 77  87  88]\n",
      "  ..., \n",
      "  [ 42  37  34]\n",
      "  [ 77  72  69]\n",
      "  [103  98  95]]]\n",
      "[[[125 136  70]\n",
      "  [145 155  92]\n",
      "  [198 208 147]\n",
      "  ..., \n",
      "  [149 155  93]\n",
      "  [154 161  94]\n",
      "  [198 203 136]]\n",
      "\n",
      " [[191 201 140]\n",
      "  [208 218 158]\n",
      "  [188 197 140]\n",
      "  ..., \n",
      "  [152 158  96]\n",
      "  [149 155  91]\n",
      "  [175 182 115]]\n",
      "\n",
      " [[212 221 166]\n",
      "  [191 200 147]\n",
      "  [217 227 175]\n",
      "  ..., \n",
      "  [163 169 109]\n",
      "  [153 159  97]\n",
      "  [157 163  99]]\n",
      "\n",
      " ..., \n",
      " [[199 208 165]\n",
      "  [204 213 170]\n",
      "  [188 196 155]\n",
      "  ..., \n",
      "  [192 186 112]\n",
      "  [212 206 130]\n",
      "  [209 203 127]]\n",
      "\n",
      " [[233 231 172]\n",
      "  [231 229 172]\n",
      "  [212 209 154]\n",
      "  ..., \n",
      "  [179 176  97]\n",
      "  [137 134  57]\n",
      "  [189 186 109]]\n",
      "\n",
      " [[211 203 140]\n",
      "  [218 210 147]\n",
      "  [202 194 132]\n",
      "  ..., \n",
      "  [104 101  22]\n",
      "  [164 162  85]\n",
      "  [143 141  64]]]\n",
      "[[[31 26  6]\n",
      "  [57 52 32]\n",
      "  [55 50 30]\n",
      "  ..., \n",
      "  [50 44 46]\n",
      "  [50 44 46]\n",
      "  [47 41 43]]\n",
      "\n",
      " [[37 32 12]\n",
      "  [48 43 23]\n",
      "  [57 52 32]\n",
      "  ..., \n",
      "  [41 35 37]\n",
      "  [41 35 37]\n",
      "  [40 34 36]]\n",
      "\n",
      " [[93 88 69]\n",
      "  [56 51 32]\n",
      "  [45 40 21]\n",
      "  ..., \n",
      "  [41 35 37]\n",
      "  [38 32 34]\n",
      "  [31 25 27]]\n",
      "\n",
      " ..., \n",
      " [[38 39 34]\n",
      "  [23 24 19]\n",
      "  [21 22 17]\n",
      "  ..., \n",
      "  [23 20 15]\n",
      "  [30 27 22]\n",
      "  [44 41 36]]\n",
      "\n",
      " [[28 29 24]\n",
      "  [21 22 17]\n",
      "  [35 36 31]\n",
      "  ..., \n",
      "  [93 82 80]\n",
      "  [37 26 24]\n",
      "  [32 21 17]]\n",
      "\n",
      " [[32 33 28]\n",
      "  [23 24 19]\n",
      "  [34 35 30]\n",
      "  ..., \n",
      "  [46 28 26]\n",
      "  [40 20 19]\n",
      "  [73 53 52]]]\n",
      "[[[214 180 155]\n",
      "  [224 190 163]\n",
      "  [224 190 163]\n",
      "  ..., \n",
      "  [108  81  62]\n",
      "  [187 158 140]\n",
      "  [101  72  54]]\n",
      "\n",
      " [[213 179 154]\n",
      "  [222 188 161]\n",
      "  [223 189 162]\n",
      "  ..., \n",
      "  [126 100  83]\n",
      "  [175 148 131]\n",
      "  [105  78  61]]\n",
      "\n",
      " [[211 177 152]\n",
      "  [221 187 160]\n",
      "  [220 189 161]\n",
      "  ..., \n",
      "  [153 132 115]\n",
      "  [164 141 125]\n",
      "  [125 102  86]]\n",
      "\n",
      " ..., \n",
      " [[111  98  66]\n",
      "  [102  89  57]\n",
      "  [ 93  82  50]\n",
      "  ..., \n",
      "  [158 137 106]\n",
      "  [153 132 101]\n",
      "  [145 124  93]]\n",
      "\n",
      " [[ 96  80  54]\n",
      "  [ 96  80  54]\n",
      "  [ 95  81  54]\n",
      "  ..., \n",
      "  [150 129 100]\n",
      "  [146 125  96]\n",
      "  [139 118  89]]\n",
      "\n",
      " [[ 85  69  44]\n",
      "  [ 94  78  53]\n",
      "  [100  86  60]\n",
      "  ..., \n",
      "  [139 118  91]\n",
      "  [138 117  90]\n",
      "  [137 116  89]]]\n",
      "[[[255 239 183]\n",
      "  [250 224 167]\n",
      "  [253 224 166]\n",
      "  ..., \n",
      "  [246 222 158]\n",
      "  [248 224 160]\n",
      "  [245 220 156]]\n",
      "\n",
      " [[245 219 162]\n",
      "  [243 217 160]\n",
      "  [254 223 166]\n",
      "  ..., \n",
      "  [241 217 153]\n",
      "  [245 220 156]\n",
      "  [242 217 153]]\n",
      "\n",
      " [[246 217 159]\n",
      "  [247 217 157]\n",
      "  [239 207 148]\n",
      "  ..., \n",
      "  [247 223 159]\n",
      "  [252 227 163]\n",
      "  [253 228 164]]\n",
      "\n",
      " ..., \n",
      " [[196 151 109]\n",
      "  [202 157 116]\n",
      "  [203 159 120]\n",
      "  ..., \n",
      "  [235 193 145]\n",
      "  [236 194 146]\n",
      "  [232 190 142]]\n",
      "\n",
      " [[220 175 133]\n",
      "  [209 166 123]\n",
      "  [194 151 109]\n",
      "  ..., \n",
      "  [232 190 142]\n",
      "  [227 185 137]\n",
      "  [218 176 128]]\n",
      "\n",
      " [[206 163 120]\n",
      "  [204 161 118]\n",
      "  [215 172 130]\n",
      "  ..., \n",
      "  [220 178 130]\n",
      "  [226 184 136]\n",
      "  [225 183 135]]]\n",
      "\n",
      "['19999.jpg', '5b6a7be5ffc6a27b91bd3210ffa2e088', 'Three Self-Portraits with a White Wall  ', 'Expressionism', 'self-portrait', '1957']\n",
      "['29999.jpg', 'cc47068929413a16aa707faefbdf4b70', 'Ganna Walska in Manon 02', 'Art Deco', 'design', '']\n",
      "['39999.jpg', '69904cf890070e9593a566394d5dece4', 'Miss Close  ', 'Expressionism', 'portrait', '1939']\n",
      "['69999.jpg', 'f44205b1eb2de981de766e0688f8cbac', 'Portrait of Balieva', 'Expressionism', 'portrait', '1925']\n",
      "['79999.jpg', 'da9ab2081b197129eeb91477d239be00', 'With an effort he looked at them as they passed', 'Neo-Romanticism', 'illustration', '']\n",
      "['89999.jpg', 'd500fe452aef7a6f90de16197a9670bf', 'Garden of Italy', 'Impressionism', 'landscape', '']\n",
      "['9999.jpg', '485d901dc4df30b128bf01cb6e229767', 'Untitled', 'Sōsaku hanga', 'cityscape', '1967']\n",
      "['99990.jpg', '3f93b217bd0dbf874f973958f1eb6df4', 'Mother Holding Her Child in a Doorway', 'Baroque', 'genre painting', '1667']\n",
      "['99992.jpg', 'f14a3a6cc3112c9e92bc6c33c88eb264', 'From An Absent One', 'Romanticism', 'genre painting', '1871']\n",
      "['99995.jpg', '8e441c5899bf3d2f3b2c493e62fb92bf', 'Urawa', 'Ukiyo-e', 'portrait', '']\n",
      "['99996.jpg', 'c56bcab4b317984013ebef5d3c4b5906', 'Gloucester Boats', 'Impressionism', 'marina', '1902']\n",
      "['99997.jpg', '3cc9a44380296d93e68b71a27643c25f', 'Portrait of Achille Emperaire', 'Romanticism', 'portrait', '1868']\n",
      "['99998.jpg', 'f0a20221a0109091e49d14c761574cd8', 'Lumberville', 'Impressionism', 'landscape', '']\n",
      "['99999.jpg', 'e1587900e782de448f604b37cde0fdfd', 'Rosa Porprina', 'Expressionism', 'portrait', '1915']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a képek betöltése és előfeldolgozása\n",
    "image_file_names, data = ppx.csv_load()\n",
    "image_name_dict, images = ppx.load_images(image_file_names)\n",
    "train_images, train_data = ppx.data_preprocess(images, data, image_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([np.asarray(ls[1]) for ls in train_images])\n",
    "labels = np.array([ls[1] for ls in train_data])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_l = encoder.transform(labels)\n",
    "\n",
    "labels_onehot = to_categorical(encoded_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 5s - loss: 0.6873     \n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 4s - loss: 0.2239     \n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 4s - loss: 0.0853     \n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 4s - loss: 0.0593     \n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 4s - loss: 0.0201     \n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 4s - loss: 0.0219     \n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 4s - loss: 0.0340     \n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 4s - loss: 0.0185     \n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 4s - loss: 0.0151     \n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 4s - loss: 0.0101     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56273828d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.asarray(train_images), labels_onehot,  batch_size=6, nb_epoch=10, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az Inception V3 konvolúciós rétegei:\n",
      "0 input_1\n",
      "1 convolution2d_1\n",
      "2 batchnormalization_1\n",
      "3 convolution2d_2\n",
      "4 batchnormalization_2\n",
      "5 convolution2d_3\n",
      "6 batchnormalization_3\n",
      "7 maxpooling2d_1\n",
      "8 convolution2d_4\n",
      "9 batchnormalization_4\n",
      "10 convolution2d_5\n",
      "11 batchnormalization_5\n",
      "12 maxpooling2d_2\n",
      "13 convolution2d_9\n",
      "14 batchnormalization_9\n",
      "15 convolution2d_7\n",
      "16 convolution2d_10\n",
      "17 batchnormalization_7\n",
      "18 batchnormalization_10\n",
      "19 averagepooling2d_1\n",
      "20 convolution2d_6\n",
      "21 convolution2d_8\n",
      "22 convolution2d_11\n",
      "23 convolution2d_12\n",
      "24 batchnormalization_6\n",
      "25 batchnormalization_8\n",
      "26 batchnormalization_11\n",
      "27 batchnormalization_12\n",
      "28 mixed0\n",
      "29 convolution2d_16\n",
      "30 batchnormalization_16\n",
      "31 convolution2d_14\n",
      "32 convolution2d_17\n",
      "33 batchnormalization_14\n",
      "34 batchnormalization_17\n",
      "35 averagepooling2d_2\n",
      "36 convolution2d_13\n",
      "37 convolution2d_15\n",
      "38 convolution2d_18\n",
      "39 convolution2d_19\n",
      "40 batchnormalization_13\n",
      "41 batchnormalization_15\n",
      "42 batchnormalization_18\n",
      "43 batchnormalization_19\n",
      "44 mixed1\n",
      "45 convolution2d_23\n",
      "46 batchnormalization_23\n",
      "47 convolution2d_21\n",
      "48 convolution2d_24\n",
      "49 batchnormalization_21\n",
      "50 batchnormalization_24\n",
      "51 averagepooling2d_3\n",
      "52 convolution2d_20\n",
      "53 convolution2d_22\n",
      "54 convolution2d_25\n",
      "55 convolution2d_26\n",
      "56 batchnormalization_20\n",
      "57 batchnormalization_22\n",
      "58 batchnormalization_25\n",
      "59 batchnormalization_26\n",
      "60 mixed2\n",
      "61 convolution2d_28\n",
      "62 batchnormalization_28\n",
      "63 convolution2d_29\n",
      "64 batchnormalization_29\n",
      "65 convolution2d_27\n",
      "66 convolution2d_30\n",
      "67 batchnormalization_27\n",
      "68 batchnormalization_30\n",
      "69 maxpooling2d_3\n",
      "70 mixed3\n",
      "71 convolution2d_35\n",
      "72 batchnormalization_35\n",
      "73 convolution2d_36\n",
      "74 batchnormalization_36\n",
      "75 convolution2d_32\n",
      "76 convolution2d_37\n",
      "77 batchnormalization_32\n",
      "78 batchnormalization_37\n",
      "79 convolution2d_33\n",
      "80 convolution2d_38\n",
      "81 batchnormalization_33\n",
      "82 batchnormalization_38\n",
      "83 averagepooling2d_4\n",
      "84 convolution2d_31\n",
      "85 convolution2d_34\n",
      "86 convolution2d_39\n",
      "87 convolution2d_40\n",
      "88 batchnormalization_31\n",
      "89 batchnormalization_34\n",
      "90 batchnormalization_39\n",
      "91 batchnormalization_40\n",
      "92 mixed4\n",
      "93 convolution2d_45\n",
      "94 batchnormalization_45\n",
      "95 convolution2d_46\n",
      "96 batchnormalization_46\n",
      "97 convolution2d_42\n",
      "98 convolution2d_47\n",
      "99 batchnormalization_42\n",
      "100 batchnormalization_47\n",
      "101 convolution2d_43\n",
      "102 convolution2d_48\n",
      "103 batchnormalization_43\n",
      "104 batchnormalization_48\n",
      "105 averagepooling2d_5\n",
      "106 convolution2d_41\n",
      "107 convolution2d_44\n",
      "108 convolution2d_49\n",
      "109 convolution2d_50\n",
      "110 batchnormalization_41\n",
      "111 batchnormalization_44\n",
      "112 batchnormalization_49\n",
      "113 batchnormalization_50\n",
      "114 mixed5\n",
      "115 convolution2d_55\n",
      "116 batchnormalization_55\n",
      "117 convolution2d_56\n",
      "118 batchnormalization_56\n",
      "119 convolution2d_52\n",
      "120 convolution2d_57\n",
      "121 batchnormalization_52\n",
      "122 batchnormalization_57\n",
      "123 convolution2d_53\n",
      "124 convolution2d_58\n",
      "125 batchnormalization_53\n",
      "126 batchnormalization_58\n",
      "127 averagepooling2d_6\n",
      "128 convolution2d_51\n",
      "129 convolution2d_54\n",
      "130 convolution2d_59\n",
      "131 convolution2d_60\n",
      "132 batchnormalization_51\n",
      "133 batchnormalization_54\n",
      "134 batchnormalization_59\n",
      "135 batchnormalization_60\n",
      "136 mixed6\n",
      "137 convolution2d_65\n",
      "138 batchnormalization_65\n",
      "139 convolution2d_66\n",
      "140 batchnormalization_66\n",
      "141 convolution2d_62\n",
      "142 convolution2d_67\n",
      "143 batchnormalization_62\n",
      "144 batchnormalization_67\n",
      "145 convolution2d_63\n",
      "146 convolution2d_68\n",
      "147 batchnormalization_63\n",
      "148 batchnormalization_68\n",
      "149 averagepooling2d_7\n",
      "150 convolution2d_61\n",
      "151 convolution2d_64\n",
      "152 convolution2d_69\n",
      "153 convolution2d_70\n",
      "154 batchnormalization_61\n",
      "155 batchnormalization_64\n",
      "156 batchnormalization_69\n",
      "157 batchnormalization_70\n",
      "158 mixed7\n",
      "159 convolution2d_73\n",
      "160 batchnormalization_73\n",
      "161 convolution2d_74\n",
      "162 batchnormalization_74\n",
      "163 convolution2d_71\n",
      "164 convolution2d_75\n",
      "165 batchnormalization_71\n",
      "166 batchnormalization_75\n",
      "167 convolution2d_72\n",
      "168 convolution2d_76\n",
      "169 batchnormalization_72\n",
      "170 batchnormalization_76\n",
      "171 averagepooling2d_8\n",
      "172 mixed8\n",
      "173 convolution2d_81\n",
      "174 batchnormalization_81\n",
      "175 convolution2d_78\n",
      "176 convolution2d_82\n",
      "177 batchnormalization_78\n",
      "178 batchnormalization_82\n",
      "179 convolution2d_79\n",
      "180 convolution2d_80\n",
      "181 convolution2d_83\n",
      "182 convolution2d_84\n",
      "183 averagepooling2d_9\n",
      "184 convolution2d_77\n",
      "185 batchnormalization_79\n",
      "186 batchnormalization_80\n",
      "187 batchnormalization_83\n",
      "188 batchnormalization_84\n",
      "189 convolution2d_85\n",
      "190 batchnormalization_77\n",
      "191 mixed9_0\n",
      "192 merge_1\n",
      "193 batchnormalization_85\n",
      "194 mixed9\n",
      "195 convolution2d_90\n",
      "196 batchnormalization_90\n",
      "197 convolution2d_87\n",
      "198 convolution2d_91\n",
      "199 batchnormalization_87\n",
      "200 batchnormalization_91\n",
      "201 convolution2d_88\n",
      "202 convolution2d_89\n",
      "203 convolution2d_92\n",
      "204 convolution2d_93\n",
      "205 averagepooling2d_10\n",
      "206 convolution2d_86\n",
      "207 batchnormalization_88\n",
      "208 batchnormalization_89\n",
      "209 batchnormalization_92\n",
      "210 batchnormalization_93\n",
      "211 convolution2d_94\n",
      "212 batchnormalization_86\n",
      "213 mixed9_1\n",
      "214 merge_2\n",
      "215 batchnormalization_94\n",
      "216 mixed10\n"
     ]
    }
   ],
   "source": [
    "# ehhez először nézzük meg a háló felépítését\n",
    "print(\"Az Inception V3 konvolúciós rétegei:\")\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# majd a hálónak csak az első 172 rétegét fagyasztjuk, a többit pedig engedjük tanulni\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 6s - loss: 0.0142     \n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 5s - loss: 0.0176     \n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 5s - loss: 0.0127     \n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 5s - loss: 0.0074     \n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 5s - loss: 0.0117     \n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 5s - loss: 0.0199     \n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 5s - loss: 0.0134     \n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 5s - loss: 0.0396     \n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 5s - loss: 0.0117     \n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 5s - loss: 0.0207     \n",
      "Tanítás vége.\n"
     ]
    }
   ],
   "source": [
    "# ez után újra le kell fordítanunk a hálót, hogy most már az Inception V3 felsőbb rétegei tanuljanak\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy')\n",
    "\n",
    "# és ismét indítunk egy tanítást, ezúttal nem csak az előrecsatolt rétegek,\n",
    "# hanem az Inception V3 felső rétegei is tovább tanulnak\n",
    "model.fit(np.asarray(train_images), labels_onehot,  batch_size=6, nb_epoch=10, callbacks=[history])\n",
    "print(\"Tanítás vége.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "0.44\n",
      "0.00\n",
      "0.90\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.01\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.57\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.01\n",
      "1.00\n",
      "0.00\n",
      "0.03\n",
      "0.00\n",
      "0.00\n",
      "0.06\n",
      "0.06\n",
      "\n",
      "0.00\n",
      "0.01\n",
      "0.00\n",
      "0.00\n",
      "1.00\n",
      "0.02\n",
      "0.01\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.03\n",
      "0.02\n",
      "0.37\n",
      "\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "1.00\n",
      "\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.00\n",
      "0.04\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.20\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.97\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.04\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.99\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "1.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "\n",
      "0.00\n",
      "0.01\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.90\n",
      "0.00\n",
      "0.03\n",
      "\n",
      "0.00\n",
      "0.16\n",
      "0.01\n",
      "0.00\n",
      "0.00\n",
      "0.01\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.00\n",
      "0.75\n",
      "0.04\n",
      "0.00\n",
      "0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pred in model.predict(np.asarray(train_images)):\n",
    "    for p in pred:\n",
    "        print('%.2f' % p)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, merge\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import preprocess as ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a képek betöltése és előfeldolgozása\n",
    "image_file_names, data = ppx.csv_load()\n",
    "image_name_dict, images = ppx.load_images(image_file_names)\n",
    "train_images, train_data = ppx.data_preprocess(images, data, image_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 256, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  9  0  5  6  7  8 10  1  4 11 12  2  3]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# adatok megfelelő formátumra hozása a keras számára\n",
    "labels = np.array([ls[1] for ls in train_data])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "\n",
    "encoded_l = encoder.transform(labels)\n",
    "print(encoded_l)\n",
    "\n",
    "labels_onehot = to_categorical(encoded_l)\n",
    "print(labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "    # Tanulási folyamat elején létrehozunk egy-egy üres listát a kinyerni kívánt metrikák tárolása céljából.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Hiba mértéke a tanító adatokon.\n",
    "        self.losses = []\n",
    "        # Hiba mértéke a validációs adatokon.\n",
    "        self.valid_losses = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a tanító adatokon. \n",
    "        self.accs = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a validációs adatokon. \n",
    "        self.valid_accs = []\n",
    "        # A tanítási fázisok sorszámozása.\n",
    "        self.epoch = 0\n",
    "    \n",
    "    # Minden egyes tanítási fázis végén mentsük el, hogy hogyan teljesít aktuálisan a háló. \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 1 == 0:\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.valid_losses.append(logs.get('val_loss'))\n",
    "            self.accs.append(logs.get('acc'))\n",
    "            self.valid_accs.append(logs.get('val_acc'))\n",
    "            self.epoch += 1\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# error esetére, elvileg nem okoz gondot 'jó' esetben sem\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "##########################################################\n",
    "\n",
    "# előtanított modell betöltése, a fully-connected rétegek nélkül\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# az utolsó konvolúciós réteg utána egy global average pooling réteget teszünk, ez rögtön \"lapítja\" (flatten) a 2D konvolúciót\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n",
      "tf\n",
      "tf\n",
      "tf\n"
     ]
    }
   ],
   "source": [
    "# kinyerjük a stílusjegyeket a cnn köztes rétegegeiből (és max pool cnn kimeneti rétegére)\n",
    "style1 = base_model.layers[54 ].output\n",
    "style1 = GlobalAveragePooling2D()(style1)\n",
    "style1 = Dense(96, activation='relu')(style1)\n",
    "style2 = base_model.layers[117].output\n",
    "style2 = GlobalAveragePooling2D()(style2)\n",
    "style2 = Dense(160, activation='relu')(style2)\n",
    "style3 = base_model.layers[184].output\n",
    "style3 = GlobalAveragePooling2D()(style3)\n",
    "style3 = Dense(320, activation='relu')(style3)\n",
    "\n",
    "style_final = base_model.output\n",
    "style_final = GlobalAveragePooling2D()(style_final)\n",
    "\n",
    "# egymás mellé tesszük a különböző szintű feature-öket\n",
    "ff = merge([style1, style2, style3, style_final], mode='concat')\n",
    "\n",
    "# ezután hozzáadunk két előrecsatolt réteget ReLU aktivációs függvénnyel\n",
    "ff = Dense(1024, activation='relu')(ff)\n",
    "ff = Dense(1024, activation='relu')(ff)\n",
    "\n",
    "# és végül egy kimenete lesz a hálónak - a \"binary_crossentropy\" költségfüggvénynek erre van szüksége\n",
    "predictions = Dense(labels_onehot.shape[1], activation='softmax')(ff)\n",
    "# a model létrehozása\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# két lépésben fogjuk tanítani a hálót\n",
    "# az első lépésben csak az előrecsatolt rétegeket tanítjuk, a konvolúciós rétegeket befagyasztjuk\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# lefordítjuk a modelt (fontos, hogy ezt a rétegek befagyasztása után csináljuk\"\n",
    "# mivel két osztályunk van, ezért bináris keresztentrópia költségfüggvényt használunk\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s - loss: 2.7504 - acc: 0.2727 - val_loss: 4.2366 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s - loss: 0.3868 - acc: 1.0000 - val_loss: 6.1345 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 9.8033 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s - loss: 0.2326 - acc: 0.9091 - val_loss: 13.1851 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s - loss: 0.3315 - acc: 0.9091 - val_loss: 15.1209 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s - loss: 0.0425 - acc: 1.0000 - val_loss: 15.4690 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s - loss: 0.2692 - acc: 0.9091 - val_loss: 15.1603 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s - loss: 4.7105e-04 - acc: 1.0000 - val_loss: 14.8492 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s - loss: 6.3067e-05 - acc: 1.0000 - val_loss: 15.1791 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s - loss: 0.4773 - acc: 0.9091 - val_loss: 15.8274 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b86915dd8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, labels_onehot, batch_size=8, nb_epoch=10, validation_split=0.2, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az Inception V3 konvolúciós rétegei:\n",
      "0 input_2\n",
      "1 convolution2d_95\n",
      "2 batchnormalization_95\n",
      "3 convolution2d_96\n",
      "4 batchnormalization_96\n",
      "5 convolution2d_97\n",
      "6 batchnormalization_97\n",
      "7 maxpooling2d_4\n",
      "8 convolution2d_98\n",
      "9 batchnormalization_98\n",
      "10 convolution2d_99\n",
      "11 batchnormalization_99\n",
      "12 maxpooling2d_5\n",
      "13 convolution2d_103\n",
      "14 batchnormalization_103\n",
      "15 convolution2d_101\n",
      "16 convolution2d_104\n",
      "17 batchnormalization_101\n",
      "18 batchnormalization_104\n",
      "19 averagepooling2d_11\n",
      "20 convolution2d_100\n",
      "21 convolution2d_102\n",
      "22 convolution2d_105\n",
      "23 convolution2d_106\n",
      "24 batchnormalization_100\n",
      "25 batchnormalization_102\n",
      "26 batchnormalization_105\n",
      "27 batchnormalization_106\n",
      "28 mixed0\n",
      "29 convolution2d_110\n",
      "30 batchnormalization_110\n",
      "31 convolution2d_108\n",
      "32 convolution2d_111\n",
      "33 batchnormalization_108\n",
      "34 batchnormalization_111\n",
      "35 averagepooling2d_12\n",
      "36 convolution2d_107\n",
      "37 convolution2d_109\n",
      "38 convolution2d_112\n",
      "39 convolution2d_113\n",
      "40 batchnormalization_107\n",
      "41 batchnormalization_109\n",
      "42 batchnormalization_112\n",
      "43 batchnormalization_113\n",
      "44 mixed1\n",
      "45 convolution2d_117\n",
      "46 batchnormalization_117\n",
      "47 convolution2d_115\n",
      "48 convolution2d_118\n",
      "49 batchnormalization_115\n",
      "50 batchnormalization_118\n",
      "51 averagepooling2d_13\n",
      "52 convolution2d_114\n",
      "53 convolution2d_116\n",
      "54 convolution2d_119\n",
      "55 convolution2d_120\n",
      "56 batchnormalization_114\n",
      "57 batchnormalization_116\n",
      "58 batchnormalization_119\n",
      "59 batchnormalization_120\n",
      "60 mixed2\n",
      "61 convolution2d_122\n",
      "62 batchnormalization_122\n",
      "63 convolution2d_123\n",
      "64 batchnormalization_123\n",
      "65 convolution2d_121\n",
      "66 convolution2d_124\n",
      "67 batchnormalization_121\n",
      "68 batchnormalization_124\n",
      "69 maxpooling2d_6\n",
      "70 mixed3\n",
      "71 convolution2d_129\n",
      "72 batchnormalization_129\n",
      "73 convolution2d_130\n",
      "74 batchnormalization_130\n",
      "75 convolution2d_126\n",
      "76 convolution2d_131\n",
      "77 batchnormalization_126\n",
      "78 batchnormalization_131\n",
      "79 convolution2d_127\n",
      "80 convolution2d_132\n",
      "81 batchnormalization_127\n",
      "82 batchnormalization_132\n",
      "83 averagepooling2d_14\n",
      "84 convolution2d_125\n",
      "85 convolution2d_128\n",
      "86 convolution2d_133\n",
      "87 convolution2d_134\n",
      "88 batchnormalization_125\n",
      "89 batchnormalization_128\n",
      "90 batchnormalization_133\n",
      "91 batchnormalization_134\n",
      "92 mixed4\n",
      "93 convolution2d_139\n",
      "94 batchnormalization_139\n",
      "95 convolution2d_140\n",
      "96 batchnormalization_140\n",
      "97 convolution2d_136\n",
      "98 convolution2d_141\n",
      "99 batchnormalization_136\n",
      "100 batchnormalization_141\n",
      "101 convolution2d_137\n",
      "102 convolution2d_142\n",
      "103 batchnormalization_137\n",
      "104 batchnormalization_142\n",
      "105 averagepooling2d_15\n",
      "106 convolution2d_135\n",
      "107 convolution2d_138\n",
      "108 convolution2d_143\n",
      "109 convolution2d_144\n",
      "110 batchnormalization_135\n",
      "111 batchnormalization_138\n",
      "112 batchnormalization_143\n",
      "113 batchnormalization_144\n",
      "114 mixed5\n",
      "115 convolution2d_149\n",
      "116 batchnormalization_149\n",
      "117 convolution2d_150\n",
      "118 batchnormalization_150\n",
      "119 convolution2d_146\n",
      "120 convolution2d_151\n",
      "121 batchnormalization_146\n",
      "122 batchnormalization_151\n",
      "123 convolution2d_147\n",
      "124 convolution2d_152\n",
      "125 batchnormalization_147\n",
      "126 batchnormalization_152\n",
      "127 averagepooling2d_16\n",
      "128 convolution2d_145\n",
      "129 convolution2d_148\n",
      "130 convolution2d_153\n",
      "131 convolution2d_154\n",
      "132 batchnormalization_145\n",
      "133 batchnormalization_148\n",
      "134 batchnormalization_153\n",
      "135 batchnormalization_154\n",
      "136 mixed6\n",
      "137 convolution2d_159\n",
      "138 batchnormalization_159\n",
      "139 convolution2d_160\n",
      "140 batchnormalization_160\n",
      "141 convolution2d_156\n",
      "142 convolution2d_161\n",
      "143 batchnormalization_156\n",
      "144 batchnormalization_161\n",
      "145 convolution2d_157\n",
      "146 convolution2d_162\n",
      "147 batchnormalization_157\n",
      "148 batchnormalization_162\n",
      "149 averagepooling2d_17\n",
      "150 convolution2d_155\n",
      "151 convolution2d_158\n",
      "152 convolution2d_163\n",
      "153 convolution2d_164\n",
      "154 batchnormalization_155\n",
      "155 batchnormalization_158\n",
      "156 batchnormalization_163\n",
      "157 batchnormalization_164\n",
      "158 mixed7\n",
      "159 convolution2d_167\n",
      "160 batchnormalization_167\n",
      "161 convolution2d_168\n",
      "162 batchnormalization_168\n",
      "163 convolution2d_165\n",
      "164 convolution2d_169\n",
      "165 batchnormalization_165\n",
      "166 batchnormalization_169\n",
      "167 convolution2d_166\n",
      "168 convolution2d_170\n",
      "169 batchnormalization_166\n",
      "170 batchnormalization_170\n",
      "171 averagepooling2d_18\n",
      "172 mixed8\n",
      "173 convolution2d_175\n",
      "174 batchnormalization_175\n",
      "175 convolution2d_172\n",
      "176 convolution2d_176\n",
      "177 batchnormalization_172\n",
      "178 batchnormalization_176\n",
      "179 convolution2d_173\n",
      "180 convolution2d_174\n",
      "181 convolution2d_177\n",
      "182 convolution2d_178\n",
      "183 averagepooling2d_19\n",
      "184 convolution2d_171\n",
      "185 batchnormalization_173\n",
      "186 batchnormalization_174\n",
      "187 batchnormalization_177\n",
      "188 batchnormalization_178\n",
      "189 convolution2d_179\n",
      "190 batchnormalization_171\n",
      "191 mixed9_0\n",
      "192 merge_6\n",
      "193 batchnormalization_179\n",
      "194 mixed9\n",
      "195 convolution2d_184\n",
      "196 batchnormalization_184\n",
      "197 convolution2d_181\n",
      "198 convolution2d_185\n",
      "199 batchnormalization_181\n",
      "200 batchnormalization_185\n",
      "201 convolution2d_182\n",
      "202 convolution2d_183\n",
      "203 convolution2d_186\n",
      "204 convolution2d_187\n",
      "205 averagepooling2d_20\n",
      "206 convolution2d_180\n",
      "207 batchnormalization_182\n",
      "208 batchnormalization_183\n",
      "209 batchnormalization_186\n",
      "210 batchnormalization_187\n",
      "211 convolution2d_188\n",
      "212 batchnormalization_180\n",
      "213 mixed9_1\n",
      "214 merge_7\n",
      "215 batchnormalization_188\n",
      "216 mixed10\n"
     ]
    }
   ],
   "source": [
    "# ehhez először nézzük meg a háló felépítését\n",
    "print(\"Az Inception V3 konvolúciós rétegei:\")\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# majd a hálónak csak az első 172 rétegét fagyasztjuk, a többit pedig engedjük tanulni\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s - loss: 6.6295e-04 - acc: 1.0000 - val_loss: 5.7277 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s - loss: 6.6149e-04 - acc: 1.0000 - val_loss: 5.7095 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s - loss: 6.5891e-04 - acc: 1.0000 - val_loss: 5.7027 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s - loss: 6.5601e-04 - acc: 1.0000 - val_loss: 5.6913 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s - loss: 6.5085e-04 - acc: 1.0000 - val_loss: 5.6730 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s - loss: 6.4653e-04 - acc: 1.0000 - val_loss: 5.6660 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s - loss: 6.4125e-04 - acc: 1.0000 - val_loss: 5.6654 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s - loss: 6.3464e-04 - acc: 1.0000 - val_loss: 5.6732 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s - loss: 6.2865e-04 - acc: 1.0000 - val_loss: 5.6806 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s - loss: 6.2145e-04 - acc: 1.0000 - val_loss: 5.6969 - val_acc: 0.0000e+00\n",
      "Tanítás vége.\n"
     ]
    }
   ],
   "source": [
    "# ez után újra le kell fordítanunk a hálót, hogy most már az Inception V3 felsőbb rétegei tanuljanak\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# és ismét indítunk egy tanítást, ezúttal nem csak az előrecsatolt rétegek,\n",
    "# hanem az Inception V3 felső rétegei is tovább tanulnak\n",
    "model.fit(train_images, labels_onehot,  batch_size=16, nb_epoch=10, validation_split=0.2, callbacks=[history])\n",
    "print(\"Tanítás vége.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_values = model.predict(np.asarray(train_images)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 4, 13, 9, 8, 2, 1, 12, 5, 6, 12, 1, 3]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred = np.zeros(predicted_values.shape, dtype=int)\n",
    "festok = []\n",
    "for i, pred in enumerate(predicted_values):\n",
    "    d = np.argmax(pred)\n",
    "    max_pred[i, d] = 1\n",
    "    festok.append(d)\n",
    "festok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5b6a7be5ffc6a27b91bd3210ffa2e088',\n",
       "       'cc47068929413a16aa707faefbdf4b70',\n",
       "       '69904cf890070e9593a566394d5dece4',\n",
       "       'f44205b1eb2de981de766e0688f8cbac',\n",
       "       'da9ab2081b197129eeb91477d239be00',\n",
       "       'd500fe452aef7a6f90de16197a9670bf',\n",
       "       '485d901dc4df30b128bf01cb6e229767',\n",
       "       '3f93b217bd0dbf874f973958f1eb6df4',\n",
       "       'f14a3a6cc3112c9e92bc6c33c88eb264',\n",
       "       '8e441c5899bf3d2f3b2c493e62fb92bf',\n",
       "       'c56bcab4b317984013ebef5d3c4b5906',\n",
       "       'f14a3a6cc3112c9e92bc6c33c88eb264',\n",
       "       '3f93b217bd0dbf874f973958f1eb6df4',\n",
       "       '5b6a7be5ffc6a27b91bd3210ffa2e088'], \n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(festok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-f5b6436ec341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# vissza kéne alakítani valahogy...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mecovered_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mohc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_features_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mohc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_indices_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# vissza kéne alakítani valahogy...\n",
    "ecovered_X = np.array([ohc.active_features_[col] for col in out.sorted_indices().indices])\\\n",
    ".reshape(n_samples, n_features) - ohc.feature_indices_[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import preprocess as ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a képek betöltése és előfeldolgozása\n",
    "image_file_names, data = ppx.csv_load()\n",
    "image_name_dict, images = ppx.load_images(image_file_names)\n",
    "train_images, train_data = ppx.data_preprocess(images, data, image_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  7  4 13  9  8  2  1 12  5  6  0 11 10]\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# adatok megfelelő formátumra hozása a keras számára\n",
    "labels = np.array([ls[1] for ls in train_data])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "\n",
    "encoded_l = encoder.transform(labels)\n",
    "print(encoded_l)\n",
    "\n",
    "labels_onehot = to_categorical(encoded_l)\n",
    "print(labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "    # Tanulási folyamat elején létrehozunk egy-egy üres listát a kinyerni kívánt metrikák tárolása céljából.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Hiba mértéke a tanító adatokon.\n",
    "        self.losses = []\n",
    "        # Hiba mértéke a validációs adatokon.\n",
    "        self.valid_losses = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a tanító adatokon. \n",
    "        self.accs = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a validációs adatokon. \n",
    "        self.valid_accs = []\n",
    "        # A tanítási fázisok sorszámozása.\n",
    "        self.epoch = 0\n",
    "    \n",
    "    # Minden egyes tanítási fázis végén mentsük el, hogy hogyan teljesít aktuálisan a háló. \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 1 == 0:\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.valid_losses.append(logs.get('val_loss'))\n",
    "            self.accs.append(logs.get('acc'))\n",
    "            self.valid_accs.append(logs.get('val_acc'))\n",
    "            self.epoch += 1\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epoch szám\n",
    "nb_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# error esetére, elvileg nem okoz gondot 'jó' esetben sem\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "##########################################################\n",
    "\n",
    "# előtanított modell betöltése, a fully-connected rétegek nélkül\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# az utolsó konvolúciós réteg utána egy global average pooling réteget teszünk, ez rögtön \"lapítja\" (flatten) a 2D konvolúciót\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# ezután hozzáadunk egy előrecsatolt réteget ReLU aktivációs függvénnyel\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# és végül egy kimenete lesz a hálónak - a \"binary_crossentropy\" költségfüggvénynek erre van szüksége\n",
    "predictions = Dense(labels_onehot.shape[1], activation='softmax')(x)\n",
    "# a model létrehozása\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# két lépésben fogjuk tanítani a hálót\n",
    "# az első lépésben csak az előrecsatolt rétegeket tanítjuk, a konvolúciós rétegeket befagyasztjuk\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# lefordítjuk a modelt (fontos, hogy ezt a rétegek befagyasztása után csináljuk\"\n",
    "# mivel két osztályunk van, ezért bináris keresztentrópia költségfüggvényt használunk\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s - loss: 2.7997 - acc: 0.0909 - val_loss: 4.7664 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s - loss: 0.3965 - acc: 0.9091 - val_loss: 4.9350 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s - loss: 0.0400 - acc: 1.0000 - val_loss: 5.1979 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s - loss: 0.0351 - acc: 1.0000 - val_loss: 5.4493 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s - loss: 5.5623e-04 - acc: 1.0000 - val_loss: 5.6827 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s - loss: 1.5444e-04 - acc: 1.0000 - val_loss: 5.8434 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s - loss: 0.1398 - acc: 0.9091 - val_loss: 6.1128 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s - loss: 0.0515 - acc: 1.0000 - val_loss: 6.4134 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s - loss: 5.5383e-04 - acc: 1.0000 - val_loss: 6.8035 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s - loss: 0.0105 - acc: 1.0000 - val_loss: 7.2251 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3fc8e2a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(imgs, labels_onehot, batch_size=8, nb_epoch=10, validation_split=0.2, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az Inception V3 konvolúciós rétegei:\n",
      "0 input_1\n",
      "1 convolution2d_1\n",
      "2 batchnormalization_1\n",
      "3 convolution2d_2\n",
      "4 batchnormalization_2\n",
      "5 convolution2d_3\n",
      "6 batchnormalization_3\n",
      "7 maxpooling2d_1\n",
      "8 convolution2d_4\n",
      "9 batchnormalization_4\n",
      "10 convolution2d_5\n",
      "11 batchnormalization_5\n",
      "12 maxpooling2d_2\n",
      "13 convolution2d_9\n",
      "14 batchnormalization_9\n",
      "15 convolution2d_7\n",
      "16 convolution2d_10\n",
      "17 batchnormalization_7\n",
      "18 batchnormalization_10\n",
      "19 averagepooling2d_1\n",
      "20 convolution2d_6\n",
      "21 convolution2d_8\n",
      "22 convolution2d_11\n",
      "23 convolution2d_12\n",
      "24 batchnormalization_6\n",
      "25 batchnormalization_8\n",
      "26 batchnormalization_11\n",
      "27 batchnormalization_12\n",
      "28 mixed0\n",
      "29 convolution2d_16\n",
      "30 batchnormalization_16\n",
      "31 convolution2d_14\n",
      "32 convolution2d_17\n",
      "33 batchnormalization_14\n",
      "34 batchnormalization_17\n",
      "35 averagepooling2d_2\n",
      "36 convolution2d_13\n",
      "37 convolution2d_15\n",
      "38 convolution2d_18\n",
      "39 convolution2d_19\n",
      "40 batchnormalization_13\n",
      "41 batchnormalization_15\n",
      "42 batchnormalization_18\n",
      "43 batchnormalization_19\n",
      "44 mixed1\n",
      "45 convolution2d_23\n",
      "46 batchnormalization_23\n",
      "47 convolution2d_21\n",
      "48 convolution2d_24\n",
      "49 batchnormalization_21\n",
      "50 batchnormalization_24\n",
      "51 averagepooling2d_3\n",
      "52 convolution2d_20\n",
      "53 convolution2d_22\n",
      "54 convolution2d_25\n",
      "55 convolution2d_26\n",
      "56 batchnormalization_20\n",
      "57 batchnormalization_22\n",
      "58 batchnormalization_25\n",
      "59 batchnormalization_26\n",
      "60 mixed2\n",
      "61 convolution2d_28\n",
      "62 batchnormalization_28\n",
      "63 convolution2d_29\n",
      "64 batchnormalization_29\n",
      "65 convolution2d_27\n",
      "66 convolution2d_30\n",
      "67 batchnormalization_27\n",
      "68 batchnormalization_30\n",
      "69 maxpooling2d_3\n",
      "70 mixed3\n",
      "71 convolution2d_35\n",
      "72 batchnormalization_35\n",
      "73 convolution2d_36\n",
      "74 batchnormalization_36\n",
      "75 convolution2d_32\n",
      "76 convolution2d_37\n",
      "77 batchnormalization_32\n",
      "78 batchnormalization_37\n",
      "79 convolution2d_33\n",
      "80 convolution2d_38\n",
      "81 batchnormalization_33\n",
      "82 batchnormalization_38\n",
      "83 averagepooling2d_4\n",
      "84 convolution2d_31\n",
      "85 convolution2d_34\n",
      "86 convolution2d_39\n",
      "87 convolution2d_40\n",
      "88 batchnormalization_31\n",
      "89 batchnormalization_34\n",
      "90 batchnormalization_39\n",
      "91 batchnormalization_40\n",
      "92 mixed4\n",
      "93 convolution2d_45\n",
      "94 batchnormalization_45\n",
      "95 convolution2d_46\n",
      "96 batchnormalization_46\n",
      "97 convolution2d_42\n",
      "98 convolution2d_47\n",
      "99 batchnormalization_42\n",
      "100 batchnormalization_47\n",
      "101 convolution2d_43\n",
      "102 convolution2d_48\n",
      "103 batchnormalization_43\n",
      "104 batchnormalization_48\n",
      "105 averagepooling2d_5\n",
      "106 convolution2d_41\n",
      "107 convolution2d_44\n",
      "108 convolution2d_49\n",
      "109 convolution2d_50\n",
      "110 batchnormalization_41\n",
      "111 batchnormalization_44\n",
      "112 batchnormalization_49\n",
      "113 batchnormalization_50\n",
      "114 mixed5\n",
      "115 convolution2d_55\n",
      "116 batchnormalization_55\n",
      "117 convolution2d_56\n",
      "118 batchnormalization_56\n",
      "119 convolution2d_52\n",
      "120 convolution2d_57\n",
      "121 batchnormalization_52\n",
      "122 batchnormalization_57\n",
      "123 convolution2d_53\n",
      "124 convolution2d_58\n",
      "125 batchnormalization_53\n",
      "126 batchnormalization_58\n",
      "127 averagepooling2d_6\n",
      "128 convolution2d_51\n",
      "129 convolution2d_54\n",
      "130 convolution2d_59\n",
      "131 convolution2d_60\n",
      "132 batchnormalization_51\n",
      "133 batchnormalization_54\n",
      "134 batchnormalization_59\n",
      "135 batchnormalization_60\n",
      "136 mixed6\n",
      "137 convolution2d_65\n",
      "138 batchnormalization_65\n",
      "139 convolution2d_66\n",
      "140 batchnormalization_66\n",
      "141 convolution2d_62\n",
      "142 convolution2d_67\n",
      "143 batchnormalization_62\n",
      "144 batchnormalization_67\n",
      "145 convolution2d_63\n",
      "146 convolution2d_68\n",
      "147 batchnormalization_63\n",
      "148 batchnormalization_68\n",
      "149 averagepooling2d_7\n",
      "150 convolution2d_61\n",
      "151 convolution2d_64\n",
      "152 convolution2d_69\n",
      "153 convolution2d_70\n",
      "154 batchnormalization_61\n",
      "155 batchnormalization_64\n",
      "156 batchnormalization_69\n",
      "157 batchnormalization_70\n",
      "158 mixed7\n",
      "159 convolution2d_73\n",
      "160 batchnormalization_73\n",
      "161 convolution2d_74\n",
      "162 batchnormalization_74\n",
      "163 convolution2d_71\n",
      "164 convolution2d_75\n",
      "165 batchnormalization_71\n",
      "166 batchnormalization_75\n",
      "167 convolution2d_72\n",
      "168 convolution2d_76\n",
      "169 batchnormalization_72\n",
      "170 batchnormalization_76\n",
      "171 averagepooling2d_8\n",
      "172 mixed8\n",
      "173 convolution2d_81\n",
      "174 batchnormalization_81\n",
      "175 convolution2d_78\n",
      "176 convolution2d_82\n",
      "177 batchnormalization_78\n",
      "178 batchnormalization_82\n",
      "179 convolution2d_79\n",
      "180 convolution2d_80\n",
      "181 convolution2d_83\n",
      "182 convolution2d_84\n",
      "183 averagepooling2d_9\n",
      "184 convolution2d_77\n",
      "185 batchnormalization_79\n",
      "186 batchnormalization_80\n",
      "187 batchnormalization_83\n",
      "188 batchnormalization_84\n",
      "189 convolution2d_85\n",
      "190 batchnormalization_77\n",
      "191 mixed9_0\n",
      "192 merge_1\n",
      "193 batchnormalization_85\n",
      "194 mixed9\n",
      "195 convolution2d_90\n",
      "196 batchnormalization_90\n",
      "197 convolution2d_87\n",
      "198 convolution2d_91\n",
      "199 batchnormalization_87\n",
      "200 batchnormalization_91\n",
      "201 convolution2d_88\n",
      "202 convolution2d_89\n",
      "203 convolution2d_92\n",
      "204 convolution2d_93\n",
      "205 averagepooling2d_10\n",
      "206 convolution2d_86\n",
      "207 batchnormalization_88\n",
      "208 batchnormalization_89\n",
      "209 batchnormalization_92\n",
      "210 batchnormalization_93\n",
      "211 convolution2d_94\n",
      "212 batchnormalization_86\n",
      "213 mixed9_1\n",
      "214 merge_2\n",
      "215 batchnormalization_94\n",
      "216 mixed10\n"
     ]
    }
   ],
   "source": [
    "# ehhez először nézzük meg a háló felépítését\n",
    "print(\"Az Inception V3 konvolúciós rétegei:\")\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# majd a hálónak csak az első 172 rétegét fagyasztjuk, a többit pedig engedjük tanulni\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.2628 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.2844 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.3210 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.3666 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.4078 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.4345 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.4729 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.5016 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.5219 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 7.5291 - val_acc: 0.0000e+00\n",
      "Tanítás vége.\n"
     ]
    }
   ],
   "source": [
    "# ez után újra le kell fordítanunk a hálót, hogy most már az Inception V3 felsőbb rétegei tanuljanak\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# és ismét indítunk egy tanítást, ezúttal nem csak az előrecsatolt rétegek,\n",
    "# hanem az Inception V3 felső rétegei is tovább tanulnak\n",
    "model.fit(np.asarray(train_images), labels_onehot,  batch_size=16, nb_epoch=10, validation_split=0.2, callbacks=[history])\n",
    "print(\"Tanítás vége.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_values = model.predict(np.asarray(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pred = np.zeros(predicted_values.shape, dtype=int)\n",
    "for i, pred in enumerate(predicted_values):\n",
    "    d = np.argmax(pred)\n",
    "    max_pred[i, d] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(max_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fd15be8f443c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mecovered_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mohc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_features_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mohc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_indices_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# vissza kéne alakítani valahogy...\n",
    "ecovered_X = np.array([ohc.active_features_[col] for col in out.sorted_indices().indices])\\\n",
    ".reshape(n_samples, n_features) - ohc.feature_indices_[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import preprocess as ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[189 180 165]\n",
      "  [181 172 157]\n",
      "  [189 180 165]\n",
      "  ..., \n",
      "  [181 176 154]\n",
      "  [164 159 137]\n",
      "  [205 200 178]]\n",
      "\n",
      " [[197 188 173]\n",
      "  [198 189 174]\n",
      "  [190 181 166]\n",
      "  ..., \n",
      "  [179 174 154]\n",
      "  [198 193 173]\n",
      "  [207 202 182]]\n",
      "\n",
      " [[207 198 183]\n",
      "  [210 201 186]\n",
      "  [192 183 168]\n",
      "  ..., \n",
      "  [187 184 165]\n",
      "  [204 201 182]\n",
      "  [189 186 167]]\n",
      "\n",
      " ..., \n",
      " [[ 99  20  25]\n",
      "  [ 93  21  24]\n",
      "  [ 72  11   8]\n",
      "  ..., \n",
      "  [  2  17  10]\n",
      "  [  5  20  13]\n",
      "  [  9  24  19]]\n",
      "\n",
      " [[107  28  33]\n",
      "  [ 96  24  27]\n",
      "  [ 69   8   5]\n",
      "  ..., \n",
      "  [  7  20  13]\n",
      "  [ 10  23  16]\n",
      "  [ 14  27  20]]\n",
      "\n",
      " [[118  39  44]\n",
      "  [107  35  38]\n",
      "  [ 85  24  21]\n",
      "  ..., \n",
      "  [ 41  54  45]\n",
      "  [ 37  50  43]\n",
      "  [ 33  46  39]]]\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "[[[220 208 168]\n",
      "  [220 208 168]\n",
      "  [219 207 167]\n",
      "  ..., \n",
      "  [123 112  66]\n",
      "  [108  97  52]\n",
      "  [ 95  84  39]]\n",
      "\n",
      " [[224 212 172]\n",
      "  [224 212 172]\n",
      "  [223 211 171]\n",
      "  ..., \n",
      "  [117 106  61]\n",
      "  [102  91  46]\n",
      "  [ 91  79  37]]\n",
      "\n",
      " [[225 213 173]\n",
      "  [224 212 172]\n",
      "  [224 212 172]\n",
      "  ..., \n",
      "  [119 108  63]\n",
      "  [106  94  52]\n",
      "  [ 96  84  42]]\n",
      "\n",
      " ..., \n",
      " [[141 138  95]\n",
      "  [143 140  97]\n",
      "  [145 142  99]\n",
      "  ..., \n",
      "  [  3   5   2]\n",
      "  [  4   6   3]\n",
      "  [  5   7   4]]\n",
      "\n",
      " [[141 138  97]\n",
      "  [142 139  98]\n",
      "  [143 140  99]\n",
      "  ..., \n",
      "  [  4   6   3]\n",
      "  [  5   7   4]\n",
      "  [  5   7   4]]\n",
      "\n",
      " [[160 156 118]\n",
      "  [158 154 116]\n",
      "  [156 152 114]\n",
      "  ..., \n",
      "  [  4   6   3]\n",
      "  [  5   7   4]\n",
      "  [  6   8   5]]]\n",
      "[[[194 138  64]\n",
      "  [203 147  73]\n",
      "  [210 154  80]\n",
      "  ..., \n",
      "  [ 93  40   0]\n",
      "  [101  47   9]\n",
      "  [104  50  12]]\n",
      "\n",
      " [[211 155  81]\n",
      "  [213 157  83]\n",
      "  [212 156  82]\n",
      "  ..., \n",
      "  [224 172 122]\n",
      "  [221 169 119]\n",
      "  [218 166 116]]\n",
      "\n",
      " [[208 152  78]\n",
      "  [212 156  82]\n",
      "  [215 159  85]\n",
      "  ..., \n",
      "  [205 155  94]\n",
      "  [209 159 100]\n",
      "  [212 162 103]]\n",
      "\n",
      " ..., \n",
      " [[ 95  77  41]\n",
      "  [ 98  80  44]\n",
      "  [ 98  80  44]\n",
      "  ..., \n",
      "  [ 81  75  53]\n",
      "  [ 75  69  47]\n",
      "  [ 79  73  51]]\n",
      "\n",
      " [[ 89  79  52]\n",
      "  [ 91  81  54]\n",
      "  [ 90  78  52]\n",
      "  ..., \n",
      "  [ 91  80  60]\n",
      "  [ 73  62  42]\n",
      "  [ 92  81  61]]\n",
      "\n",
      " [[ 82  72  45]\n",
      "  [ 81  71  44]\n",
      "  [ 79  67  41]\n",
      "  ..., \n",
      "  [ 91  80  60]\n",
      "  [ 69  58  38]\n",
      "  [ 86  75  55]]]\n",
      "[[[251 233 213]\n",
      "  [251 233 213]\n",
      "  [251 233 213]\n",
      "  ..., \n",
      "  [250 235 212]\n",
      "  [250 235 212]\n",
      "  [250 235 212]]\n",
      "\n",
      " [[250 233 213]\n",
      "  [250 233 213]\n",
      "  [250 233 213]\n",
      "  ..., \n",
      "  [250 235 212]\n",
      "  [250 235 212]\n",
      "  [250 235 212]]\n",
      "\n",
      " [[251 234 214]\n",
      "  [251 234 214]\n",
      "  [251 234 214]\n",
      "  ..., \n",
      "  [251 236 213]\n",
      "  [251 236 213]\n",
      "  [251 236 213]]\n",
      "\n",
      " ..., \n",
      " [[248 233 214]\n",
      "  [249 234 215]\n",
      "  [250 235 216]\n",
      "  ..., \n",
      "  [249 238 220]\n",
      "  [249 238 220]\n",
      "  [249 238 220]]\n",
      "\n",
      " [[248 233 214]\n",
      "  [249 234 215]\n",
      "  [250 235 216]\n",
      "  ..., \n",
      "  [249 238 220]\n",
      "  [249 238 220]\n",
      "  [249 238 220]]\n",
      "\n",
      " [[248 233 214]\n",
      "  [249 234 215]\n",
      "  [250 235 216]\n",
      "  ..., \n",
      "  [249 238 220]\n",
      "  [249 238 220]\n",
      "  [249 238 220]]]\n",
      "[[[131 132  98]\n",
      "  [134 128  92]\n",
      "  [174 159 120]\n",
      "  ..., \n",
      "  [ 52  69  61]\n",
      "  [ 42  68  57]\n",
      "  [ 42  72  60]]\n",
      "\n",
      " [[116 109  67]\n",
      "  [153 140  96]\n",
      "  [194 173 128]\n",
      "  ..., \n",
      "  [ 47  62  55]\n",
      "  [ 43  64  55]\n",
      "  [ 41  67  56]]\n",
      "\n",
      " [[155 141  92]\n",
      "  [119 101  51]\n",
      "  [224 198 147]\n",
      "  ..., \n",
      "  [ 43  56  49]\n",
      "  [ 36  56  47]\n",
      "  [ 33  54  45]]\n",
      "\n",
      " ..., \n",
      " [[162 117  78]\n",
      "  [164 119  80]\n",
      "  [150 105  64]\n",
      "  ..., \n",
      "  [164 137  94]\n",
      "  [122  99  55]\n",
      "  [102  83  40]]\n",
      "\n",
      " [[161 116  77]\n",
      "  [167 122  83]\n",
      "  [159 114  73]\n",
      "  ..., \n",
      "  [235 208 165]\n",
      "  [210 187 143]\n",
      "  [ 70  51   8]]\n",
      "\n",
      " [[161 116  77]\n",
      "  [164 119  80]\n",
      "  [145 100  59]\n",
      "  ..., \n",
      "  [220 193 150]\n",
      "  [223 200 156]\n",
      "  [ 81  62  19]]]\n",
      "[[[ 95  80  83]\n",
      "  [ 73  58  61]\n",
      "  [104  89  92]\n",
      "  ..., \n",
      "  [243 234 237]\n",
      "  [241 232 235]\n",
      "  [242 233 236]]\n",
      "\n",
      " [[ 53  38  41]\n",
      "  [ 79  64  67]\n",
      "  [ 81  66  69]\n",
      "  ..., \n",
      "  [ 32  23  26]\n",
      "  [ 49  40  43]\n",
      "  [ 67  58  61]]\n",
      "\n",
      " [[ 85  73  75]\n",
      "  [ 70  58  60]\n",
      "  [ 85  73  75]\n",
      "  ..., \n",
      "  [ 49  40  43]\n",
      "  [ 65  56  59]\n",
      "  [ 71  62  65]]\n",
      "\n",
      " ..., \n",
      " [[219 204 211]\n",
      "  [ 46  31  38]\n",
      "  [108  97 103]\n",
      "  ..., \n",
      "  [ 38  38  40]\n",
      "  [ 50  50  52]\n",
      "  [ 43  43  45]]\n",
      "\n",
      " [[156 141 148]\n",
      "  [ 81  66  73]\n",
      "  [ 83  72  78]\n",
      "  ..., \n",
      "  [ 35  35  37]\n",
      "  [ 48  48  50]\n",
      "  [ 42  42  44]]\n",
      "\n",
      " [[166 149 157]\n",
      "  [ 87  72  79]\n",
      "  [ 82  71  77]\n",
      "  ..., \n",
      "  [ 27  27  29]\n",
      "  [ 49  49  51]\n",
      "  [ 46  46  48]]]\n",
      "[[[ 50  49  29]\n",
      "  [ 58  57  37]\n",
      "  [ 58  57  37]\n",
      "  ..., \n",
      "  [ 19  15  14]\n",
      "  [ 20  14  14]\n",
      "  [ 48  42  44]]\n",
      "\n",
      " [[ 45  44  26]\n",
      "  [ 53  52  34]\n",
      "  [ 88  87  69]\n",
      "  ..., \n",
      "  [ 17  13  12]\n",
      "  [ 21  15  17]\n",
      "  [ 39  33  35]]\n",
      "\n",
      " [[ 67  65  52]\n",
      "  [ 39  37  24]\n",
      "  [ 57  55  42]\n",
      "  ..., \n",
      "  [ 17  13  14]\n",
      "  [ 29  24  28]\n",
      "  [ 24  19  23]]\n",
      "\n",
      " ..., \n",
      " [[ 48  32  17]\n",
      "  [ 59  43  28]\n",
      "  [ 72  58  45]\n",
      "  ..., \n",
      "  [ 10   8  13]\n",
      "  [ 13   8  12]\n",
      "  [ 14   9  13]]\n",
      "\n",
      " [[ 35  21  12]\n",
      "  [ 47  33  24]\n",
      "  [ 41  28  20]\n",
      "  ..., \n",
      "  [ 19  17  18]\n",
      "  [ 38  34  33]\n",
      "  [ 14   9   6]]\n",
      "\n",
      " [[101  86  79]\n",
      "  [ 15   0   0]\n",
      "  [ 20   7   1]\n",
      "  ..., \n",
      "  [ 26  25  21]\n",
      "  [ 52  49  44]\n",
      "  [ 17  12   8]]]\n",
      "[[[ 98  72  35]\n",
      "  [ 96  76  43]\n",
      "  [ 97  81  56]\n",
      "  ..., \n",
      "  [ 66  71  65]\n",
      "  [ 55  60  54]\n",
      "  [ 57  62  56]]\n",
      "\n",
      " [[197 171 136]\n",
      "  [112  92  59]\n",
      "  [111  95  70]\n",
      "  ..., \n",
      "  [ 57  62  56]\n",
      "  [ 52  57  51]\n",
      "  [ 61  66  60]]\n",
      "\n",
      " [[150 124  89]\n",
      "  [160 139 108]\n",
      "  [ 83  67  42]\n",
      "  ..., \n",
      "  [ 55  60  56]\n",
      "  [ 55  60  54]\n",
      "  [ 65  70  64]]\n",
      "\n",
      " ..., \n",
      " [[183 160 126]\n",
      "  [185 162 128]\n",
      "  [164 141 107]\n",
      "  ..., \n",
      "  [236 193 138]\n",
      "  [234 193 137]\n",
      "  [231 190 134]]\n",
      "\n",
      " [[163 143 108]\n",
      "  [177 157 122]\n",
      "  [173 153 118]\n",
      "  ..., \n",
      "  [237 191 139]\n",
      "  [233 190 137]\n",
      "  [237 194 143]]\n",
      "\n",
      " [[148 128  93]\n",
      "  [153 133  98]\n",
      "  [144 124  89]\n",
      "  ..., \n",
      "  [234 186 138]\n",
      "  [234 188 139]\n",
      "  [241 195 146]]]\n",
      "[[[ 33  32  28]\n",
      "  [ 21  20  16]\n",
      "  [ 17  16  14]\n",
      "  ..., \n",
      "  [187  74  32]\n",
      "  [214  78  28]\n",
      "  [217  66  11]]\n",
      "\n",
      " [[ 29  25  22]\n",
      "  [ 27  23  20]\n",
      "  [ 33  29  26]\n",
      "  ..., \n",
      "  [173  56  36]\n",
      "  [192  51  23]\n",
      "  [255 130  99]]\n",
      "\n",
      " [[ 29  26  21]\n",
      "  [ 24  20  17]\n",
      "  [ 31  26  23]\n",
      "  ..., \n",
      "  [194  83  74]\n",
      "  [179  47  34]\n",
      "  [198  53  36]]\n",
      "\n",
      " ..., \n",
      " [[100  84  61]\n",
      "  [ 64  61  52]\n",
      "  [103 109 109]\n",
      "  ..., \n",
      "  [ 38  32  32]\n",
      "  [ 74  68  68]\n",
      "  [ 91  85  85]]\n",
      "\n",
      " [[ 75  59  36]\n",
      "  [ 82  79  70]\n",
      "  [ 53  62  61]\n",
      "  ..., \n",
      "  [ 38  32  32]\n",
      "  [ 92  86  86]\n",
      "  [ 96  90  90]]\n",
      "\n",
      " [[ 92  76  53]\n",
      "  [ 66  66  58]\n",
      "  [ 98 108 110]\n",
      "  ..., \n",
      "  [ 46  41  38]\n",
      "  [ 55  50  47]\n",
      "  [113 108 105]]]\n",
      "[[[125 136  70]\n",
      "  [145 155  92]\n",
      "  [198 208 147]\n",
      "  ..., \n",
      "  [149 155  93]\n",
      "  [154 161  94]\n",
      "  [198 203 136]]\n",
      "\n",
      " [[212 221 166]\n",
      "  [191 200 147]\n",
      "  [217 227 175]\n",
      "  ..., \n",
      "  [163 169 109]\n",
      "  [153 159  97]\n",
      "  [157 163  99]]\n",
      "\n",
      " [[176 185 138]\n",
      "  [195 204 159]\n",
      "  [198 207 164]\n",
      "  ..., \n",
      "  [177 182 124]\n",
      "  [166 172 112]\n",
      "  [155 161  99]]\n",
      "\n",
      " ..., \n",
      " [[222 224 174]\n",
      "  [217 219 169]\n",
      "  [206 208 159]\n",
      "  ..., \n",
      "  [194 188 112]\n",
      "  [222 219 142]\n",
      "  [205 202 125]]\n",
      "\n",
      " [[233 231 172]\n",
      "  [231 229 172]\n",
      "  [212 209 154]\n",
      "  ..., \n",
      "  [179 176  97]\n",
      "  [137 134  57]\n",
      "  [189 186 109]]\n",
      "\n",
      " [[231 222 155]\n",
      "  [209 200 135]\n",
      "  [161 151  89]\n",
      "  ..., \n",
      "  [190 187 108]\n",
      "  [128 126  49]\n",
      "  [168 166  89]]]\n",
      "[[[31 26  6]\n",
      "  [57 52 32]\n",
      "  [55 50 30]\n",
      "  ..., \n",
      "  [50 44 46]\n",
      "  [50 44 46]\n",
      "  [47 41 43]]\n",
      "\n",
      " [[37 32 12]\n",
      "  [48 43 23]\n",
      "  [57 52 32]\n",
      "  ..., \n",
      "  [41 35 37]\n",
      "  [41 35 37]\n",
      "  [40 34 36]]\n",
      "\n",
      " [[93 88 69]\n",
      "  [56 51 32]\n",
      "  [45 40 21]\n",
      "  ..., \n",
      "  [41 35 37]\n",
      "  [38 32 34]\n",
      "  [31 25 27]]\n",
      "\n",
      " ..., \n",
      " [[28 29 24]\n",
      "  [21 22 17]\n",
      "  [35 36 31]\n",
      "  ..., \n",
      "  [93 82 80]\n",
      "  [37 26 24]\n",
      "  [32 21 17]]\n",
      "\n",
      " [[32 33 28]\n",
      "  [23 24 19]\n",
      "  [34 35 30]\n",
      "  ..., \n",
      "  [46 28 26]\n",
      "  [40 20 19]\n",
      "  [73 53 52]]\n",
      "\n",
      " [[37 39 36]\n",
      "  [25 27 24]\n",
      "  [33 35 32]\n",
      "  ..., \n",
      "  [63 39 39]\n",
      "  [51 27 27]\n",
      "  [48 22 23]]]\n",
      "[[[214 180 155]\n",
      "  [224 190 163]\n",
      "  [224 190 163]\n",
      "  ..., \n",
      "  [187 158 140]\n",
      "  [101  72  54]\n",
      "  [139 108  90]]\n",
      "\n",
      " [[213 179 154]\n",
      "  [222 188 161]\n",
      "  [223 189 162]\n",
      "  ..., \n",
      "  [175 148 131]\n",
      "  [105  78  61]\n",
      "  [128  99  83]]\n",
      "\n",
      " [[211 177 152]\n",
      "  [221 187 160]\n",
      "  [220 189 161]\n",
      "  ..., \n",
      "  [164 141 125]\n",
      "  [125 102  86]\n",
      "  [132 106  91]]\n",
      "\n",
      " ..., \n",
      " [[111  98  66]\n",
      "  [102  89  57]\n",
      "  [ 93  82  50]\n",
      "  ..., \n",
      "  [153 132 101]\n",
      "  [145 124  93]\n",
      "  [137 116  85]]\n",
      "\n",
      " [[ 96  80  54]\n",
      "  [ 96  80  54]\n",
      "  [ 95  81  54]\n",
      "  ..., \n",
      "  [146 125  96]\n",
      "  [139 118  89]\n",
      "  [131 110  81]]\n",
      "\n",
      " [[ 85  69  44]\n",
      "  [ 94  78  53]\n",
      "  [100  86  60]\n",
      "  ..., \n",
      "  [138 117  90]\n",
      "  [137 116  89]\n",
      "  [136 115  88]]]\n",
      "[[[255 239 183]\n",
      "  [250 224 167]\n",
      "  [253 224 166]\n",
      "  ..., \n",
      "  [246 222 158]\n",
      "  [248 224 160]\n",
      "  [245 220 156]]\n",
      "\n",
      " [[244 218 161]\n",
      "  [248 219 161]\n",
      "  [248 217 160]\n",
      "  ..., \n",
      "  [241 217 153]\n",
      "  [245 220 156]\n",
      "  [243 218 154]]\n",
      "\n",
      " [[236 206 146]\n",
      "  [239 207 148]\n",
      "  [237 203 142]\n",
      "  ..., \n",
      "  [255 231 167]\n",
      "  [255 235 171]\n",
      "  [255 237 173]]\n",
      "\n",
      " ..., \n",
      " [[196 151 109]\n",
      "  [202 157 116]\n",
      "  [203 159 120]\n",
      "  ..., \n",
      "  [235 193 145]\n",
      "  [236 194 146]\n",
      "  [232 190 142]]\n",
      "\n",
      " [[196 153 110]\n",
      "  [200 157 114]\n",
      "  [201 158 116]\n",
      "  ..., \n",
      "  [227 185 137]\n",
      "  [217 175 127]\n",
      "  [204 162 114]]\n",
      "\n",
      " [[205 162 120]\n",
      "  [200 157 115]\n",
      "  [210 166 127]\n",
      "  ..., \n",
      "  [227 185 137]\n",
      "  [229 187 139]\n",
      "  [227 185 137]]]\n",
      "\n",
      "['19999.jpg', '5b6a7be5ffc6a27b91bd3210ffa2e088', 'Three Self-Portraits with a White Wall  ', 'Expressionism', 'self-portrait', '1957']\n",
      "['29999.jpg', 'cc47068929413a16aa707faefbdf4b70', 'Ganna Walska in Manon 02', 'Art Deco', 'design', '']\n",
      "['39999.jpg', '69904cf890070e9593a566394d5dece4', 'Miss Close  ', 'Expressionism', 'portrait', '1939']\n",
      "['69999.jpg', 'f44205b1eb2de981de766e0688f8cbac', 'Portrait of Balieva', 'Expressionism', 'portrait', '1925']\n",
      "['79999.jpg', 'da9ab2081b197129eeb91477d239be00', 'With an effort he looked at them as they passed', 'Neo-Romanticism', 'illustration', '']\n",
      "['89999.jpg', 'd500fe452aef7a6f90de16197a9670bf', 'Garden of Italy', 'Impressionism', 'landscape', '']\n",
      "['9999.jpg', '485d901dc4df30b128bf01cb6e229767', 'Untitled', 'Sōsaku hanga', 'cityscape', '1967']\n",
      "['99990.jpg', '3f93b217bd0dbf874f973958f1eb6df4', 'Mother Holding Her Child in a Doorway', 'Baroque', 'genre painting', '1667']\n",
      "['99992.jpg', 'f14a3a6cc3112c9e92bc6c33c88eb264', 'From An Absent One', 'Romanticism', 'genre painting', '1871']\n",
      "['99995.jpg', '8e441c5899bf3d2f3b2c493e62fb92bf', 'Urawa', 'Ukiyo-e', 'portrait', '']\n",
      "['99996.jpg', 'c56bcab4b317984013ebef5d3c4b5906', 'Gloucester Boats', 'Impressionism', 'marina', '1902']\n",
      "['99997.jpg', '3cc9a44380296d93e68b71a27643c25f', 'Portrait of Achille Emperaire', 'Romanticism', 'portrait', '1868']\n",
      "['99998.jpg', 'f0a20221a0109091e49d14c761574cd8', 'Lumberville', 'Impressionism', 'landscape', '']\n",
      "['99999.jpg', 'e1587900e782de448f604b37cde0fdfd', 'Rosa Porprina', 'Expressionism', 'portrait', '1915']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a képek betöltése és előfeldolgozása\n",
    "image_file_names, data = ppx.csv_load()\n",
    "image_name_dict, images = ppx.load_images(image_file_names)\n",
    "train_images, train_data = ppx.data_preprocess(images, data, image_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  7  4 13  9  8  2  1 12  5  6  0 11 10]\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([ls[1] for ls in train_data])\n",
    "\n",
    "\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "\n",
    "\n",
    "encoded_l = encoder.transform(labels)\n",
    "print(encoded_l)\n",
    "\n",
    "labels_onehot = to_categorical(encoded_l)\n",
    "\n",
    "print(labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "    # Tanulási folyamat elején létrehozunk egy-egy üres listát a kinyerni kívánt metrikák tárolása céljából.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Hiba mértéke a tanító adatokon.\n",
    "        self.losses = []\n",
    "        # Hiba mértéke a validációs adatokon.\n",
    "        self.valid_losses = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a tanító adatokon. \n",
    "        self.accs = []\n",
    "        # A modell jóságát, pontosságát mérő mutatószám a validációs adatokon. \n",
    "        self.valid_accs = []\n",
    "        # A tanítási fázisok sorszámozása.\n",
    "        self.epoch = 0\n",
    "    \n",
    "    # Minden egyes tanítási fázis végén mentsük el, hogy hogyan teljesít aktuálisan a háló. \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 1 == 0:\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.valid_losses.append(logs.get('val_loss'))\n",
    "            self.accs.append(logs.get('acc'))\n",
    "            self.valid_accs.append(logs.get('val_acc'))\n",
    "            self.epoch += 1\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epoch szám\n",
    "nb_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# error esetére, elvileg nem okoz gondot 'jó' esetben sem\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "##########################################################\n",
    "\n",
    "\n",
    "\n",
    "# előtanított modell betöltése, a fully-connected rétegek nélkül\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# az utolsó konvolúciós réteg utána egy global average pooling réteget teszünk, ez rögtön \"lapítja\" (flatten) a 2D konvolúciót\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# ezután hozzáadunk egy előrecsatolt réteget ReLU aktivációs függvénnyel\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# és végül egy kimenete lesz a hálónak - a \"binary_crossentropy\" költségfüggvénynek erre van szüksége\n",
    "predictions = Dense(labels_onehot.shape[1], activation='softmax')(x)\n",
    "# a model létrehozása\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# két lépésben fogjuk tanítani a hálót\n",
    "# az első lépésben csak az előrecsatolt rétegeket tanítjuk, a konvolúciós rétegeket befagyasztjuk\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# lefordítjuk a modelt (fontos, hogy ezt a rétegek befagyasztása után csináljuk\"\n",
    "# mivel két osztályunk van, ezért bináris keresztentrópia költségfüggvényt használunk\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s - loss: 3.0541 - acc: 0.0000e+00 - val_loss: 2.7648 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s - loss: 0.4502 - acc: 0.9091 - val_loss: 3.7799 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 2s - loss: 0.0587 - acc: 1.0000 - val_loss: 4.9508 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s - loss: 0.0175 - acc: 1.0000 - val_loss: 5.4750 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s - loss: 0.2147 - acc: 0.9091 - val_loss: 5.7674 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 2s - loss: 3.0597e-04 - acc: 1.0000 - val_loss: 5.8782 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s - loss: 9.7379e-04 - acc: 1.0000 - val_loss: 5.9492 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s - loss: 0.0218 - acc: 1.0000 - val_loss: 6.0415 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 2s - loss: 0.0861 - acc: 1.0000 - val_loss: 6.3274 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 2s - loss: 1.6660e-04 - acc: 1.0000 - val_loss: 6.9256 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2512443cc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(imgs, labels_onehot, batch_size=8, nb_epoch=10, validation_split=0.2, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az Inception V3 konvolúciós rétegei:\n",
      "0 input_1\n",
      "1 convolution2d_1\n",
      "2 batchnormalization_1\n",
      "3 convolution2d_2\n",
      "4 batchnormalization_2\n",
      "5 convolution2d_3\n",
      "6 batchnormalization_3\n",
      "7 maxpooling2d_1\n",
      "8 convolution2d_4\n",
      "9 batchnormalization_4\n",
      "10 convolution2d_5\n",
      "11 batchnormalization_5\n",
      "12 maxpooling2d_2\n",
      "13 convolution2d_9\n",
      "14 batchnormalization_9\n",
      "15 convolution2d_7\n",
      "16 convolution2d_10\n",
      "17 batchnormalization_7\n",
      "18 batchnormalization_10\n",
      "19 averagepooling2d_1\n",
      "20 convolution2d_6\n",
      "21 convolution2d_8\n",
      "22 convolution2d_11\n",
      "23 convolution2d_12\n",
      "24 batchnormalization_6\n",
      "25 batchnormalization_8\n",
      "26 batchnormalization_11\n",
      "27 batchnormalization_12\n",
      "28 mixed0\n",
      "29 convolution2d_16\n",
      "30 batchnormalization_16\n",
      "31 convolution2d_14\n",
      "32 convolution2d_17\n",
      "33 batchnormalization_14\n",
      "34 batchnormalization_17\n",
      "35 averagepooling2d_2\n",
      "36 convolution2d_13\n",
      "37 convolution2d_15\n",
      "38 convolution2d_18\n",
      "39 convolution2d_19\n",
      "40 batchnormalization_13\n",
      "41 batchnormalization_15\n",
      "42 batchnormalization_18\n",
      "43 batchnormalization_19\n",
      "44 mixed1\n",
      "45 convolution2d_23\n",
      "46 batchnormalization_23\n",
      "47 convolution2d_21\n",
      "48 convolution2d_24\n",
      "49 batchnormalization_21\n",
      "50 batchnormalization_24\n",
      "51 averagepooling2d_3\n",
      "52 convolution2d_20\n",
      "53 convolution2d_22\n",
      "54 convolution2d_25\n",
      "55 convolution2d_26\n",
      "56 batchnormalization_20\n",
      "57 batchnormalization_22\n",
      "58 batchnormalization_25\n",
      "59 batchnormalization_26\n",
      "60 mixed2\n",
      "61 convolution2d_28\n",
      "62 batchnormalization_28\n",
      "63 convolution2d_29\n",
      "64 batchnormalization_29\n",
      "65 convolution2d_27\n",
      "66 convolution2d_30\n",
      "67 batchnormalization_27\n",
      "68 batchnormalization_30\n",
      "69 maxpooling2d_3\n",
      "70 mixed3\n",
      "71 convolution2d_35\n",
      "72 batchnormalization_35\n",
      "73 convolution2d_36\n",
      "74 batchnormalization_36\n",
      "75 convolution2d_32\n",
      "76 convolution2d_37\n",
      "77 batchnormalization_32\n",
      "78 batchnormalization_37\n",
      "79 convolution2d_33\n",
      "80 convolution2d_38\n",
      "81 batchnormalization_33\n",
      "82 batchnormalization_38\n",
      "83 averagepooling2d_4\n",
      "84 convolution2d_31\n",
      "85 convolution2d_34\n",
      "86 convolution2d_39\n",
      "87 convolution2d_40\n",
      "88 batchnormalization_31\n",
      "89 batchnormalization_34\n",
      "90 batchnormalization_39\n",
      "91 batchnormalization_40\n",
      "92 mixed4\n",
      "93 convolution2d_45\n",
      "94 batchnormalization_45\n",
      "95 convolution2d_46\n",
      "96 batchnormalization_46\n",
      "97 convolution2d_42\n",
      "98 convolution2d_47\n",
      "99 batchnormalization_42\n",
      "100 batchnormalization_47\n",
      "101 convolution2d_43\n",
      "102 convolution2d_48\n",
      "103 batchnormalization_43\n",
      "104 batchnormalization_48\n",
      "105 averagepooling2d_5\n",
      "106 convolution2d_41\n",
      "107 convolution2d_44\n",
      "108 convolution2d_49\n",
      "109 convolution2d_50\n",
      "110 batchnormalization_41\n",
      "111 batchnormalization_44\n",
      "112 batchnormalization_49\n",
      "113 batchnormalization_50\n",
      "114 mixed5\n",
      "115 convolution2d_55\n",
      "116 batchnormalization_55\n",
      "117 convolution2d_56\n",
      "118 batchnormalization_56\n",
      "119 convolution2d_52\n",
      "120 convolution2d_57\n",
      "121 batchnormalization_52\n",
      "122 batchnormalization_57\n",
      "123 convolution2d_53\n",
      "124 convolution2d_58\n",
      "125 batchnormalization_53\n",
      "126 batchnormalization_58\n",
      "127 averagepooling2d_6\n",
      "128 convolution2d_51\n",
      "129 convolution2d_54\n",
      "130 convolution2d_59\n",
      "131 convolution2d_60\n",
      "132 batchnormalization_51\n",
      "133 batchnormalization_54\n",
      "134 batchnormalization_59\n",
      "135 batchnormalization_60\n",
      "136 mixed6\n",
      "137 convolution2d_65\n",
      "138 batchnormalization_65\n",
      "139 convolution2d_66\n",
      "140 batchnormalization_66\n",
      "141 convolution2d_62\n",
      "142 convolution2d_67\n",
      "143 batchnormalization_62\n",
      "144 batchnormalization_67\n",
      "145 convolution2d_63\n",
      "146 convolution2d_68\n",
      "147 batchnormalization_63\n",
      "148 batchnormalization_68\n",
      "149 averagepooling2d_7\n",
      "150 convolution2d_61\n",
      "151 convolution2d_64\n",
      "152 convolution2d_69\n",
      "153 convolution2d_70\n",
      "154 batchnormalization_61\n",
      "155 batchnormalization_64\n",
      "156 batchnormalization_69\n",
      "157 batchnormalization_70\n",
      "158 mixed7\n",
      "159 convolution2d_73\n",
      "160 batchnormalization_73\n",
      "161 convolution2d_74\n",
      "162 batchnormalization_74\n",
      "163 convolution2d_71\n",
      "164 convolution2d_75\n",
      "165 batchnormalization_71\n",
      "166 batchnormalization_75\n",
      "167 convolution2d_72\n",
      "168 convolution2d_76\n",
      "169 batchnormalization_72\n",
      "170 batchnormalization_76\n",
      "171 averagepooling2d_8\n",
      "172 mixed8\n",
      "173 convolution2d_81\n",
      "174 batchnormalization_81\n",
      "175 convolution2d_78\n",
      "176 convolution2d_82\n",
      "177 batchnormalization_78\n",
      "178 batchnormalization_82\n",
      "179 convolution2d_79\n",
      "180 convolution2d_80\n",
      "181 convolution2d_83\n",
      "182 convolution2d_84\n",
      "183 averagepooling2d_9\n",
      "184 convolution2d_77\n",
      "185 batchnormalization_79\n",
      "186 batchnormalization_80\n",
      "187 batchnormalization_83\n",
      "188 batchnormalization_84\n",
      "189 convolution2d_85\n",
      "190 batchnormalization_77\n",
      "191 mixed9_0\n",
      "192 merge_1\n",
      "193 batchnormalization_85\n",
      "194 mixed9\n",
      "195 convolution2d_90\n",
      "196 batchnormalization_90\n",
      "197 convolution2d_87\n",
      "198 convolution2d_91\n",
      "199 batchnormalization_87\n",
      "200 batchnormalization_91\n",
      "201 convolution2d_88\n",
      "202 convolution2d_89\n",
      "203 convolution2d_92\n",
      "204 convolution2d_93\n",
      "205 averagepooling2d_10\n",
      "206 convolution2d_86\n",
      "207 batchnormalization_88\n",
      "208 batchnormalization_89\n",
      "209 batchnormalization_92\n",
      "210 batchnormalization_93\n",
      "211 convolution2d_94\n",
      "212 batchnormalization_86\n",
      "213 mixed9_1\n",
      "214 merge_2\n",
      "215 batchnormalization_94\n",
      "216 mixed10\n"
     ]
    }
   ],
   "source": [
    "# ehhez először nézzük meg a háló felépítését\n",
    "print(\"Az Inception V3 konvolúciós rétegei:\")\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# majd a hálónak csak az első 172 rétegét fagyasztjuk, a többit pedig engedjük tanulni\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 4s - loss: 5.1532e-06 - acc: 1.0000 - val_loss: 7.1752 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 2s - loss: 5.1694e-06 - acc: 1.0000 - val_loss: 7.4286 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 3s - loss: 5.1640e-06 - acc: 1.0000 - val_loss: 7.6748 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 2s - loss: 5.1478e-06 - acc: 1.0000 - val_loss: 7.8993 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 2s - loss: 5.1478e-06 - acc: 1.0000 - val_loss: 8.1275 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 3s - loss: 5.1532e-06 - acc: 1.0000 - val_loss: 8.3686 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 2s - loss: 5.1640e-06 - acc: 1.0000 - val_loss: 8.6247 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 2s - loss: 5.1478e-06 - acc: 1.0000 - val_loss: 8.8922 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 3s - loss: 5.1478e-06 - acc: 1.0000 - val_loss: 9.1686 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 3s - loss: 5.1478e-06 - acc: 1.0000 - val_loss: 9.4515 - val_acc: 0.0000e+00\n",
      "Tanítás vége.\n"
     ]
    }
   ],
   "source": [
    "# ez után újra le kell fordítanunk a hálót, hogy most már az Inception V3 felsőbb rétegei tanuljanak\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# és ismét indítunk egy tanítást, ezúttal nem csak az előrecsatolt rétegek,\n",
    "# hanem az Inception V3 felső rétegei is tovább tanulnak\n",
    "model.fit(np.asarray(train_images), labels_onehot,  batch_size=16, nb_epoch=10, validation_split=0.2, callbacks=[history])\n",
    "print(\"Tanítás vége.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_values = model.predict(np.asarray(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(max_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_pred = np.zeros(predicted_values.shape, dtype=int)\n",
    "for i, pred in enumerate(predicted_values):\n",
    "    d = np.argmax(pred)\n",
    "    max_pred[i, d] = 1\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecovered_X = np.array([ohc.active_features_[col] for col in out.sorted_indices().indices])\n",
    "                .reshape(n_samples, n_features) - ohc.feature_indices_[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3,4), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.array([4,56,67.3,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 459)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
